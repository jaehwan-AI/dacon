{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:05:48.169752Z",
     "start_time": "2020-08-25T02:05:41.660778Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import concatenate, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# seed\n",
    "import os\n",
    "seed = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "#tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:41.955Z"
    }
   },
   "outputs": [],
   "source": [
    "train = np.load('data/train.npy', allow_pickle = 'True')\n",
    "test = np.load('data/test.npy', allow_pickle = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:42.098Z"
    }
   },
   "outputs": [],
   "source": [
    "x = train[:,2:]\n",
    "x = np.reshape(x, (-1, 28, 28, 1))\n",
    "\n",
    "x_letter = train[:,1]\n",
    "x_letter = np.reshape(x_letter, (-1, 1))\n",
    "en = OneHotEncoder()\n",
    "x_letter = en.fit_transform(x_letter).toarray()\n",
    "\n",
    "y = train[:,0]\n",
    "y = np.reshape(y, (-1, 1))\n",
    "en = OneHotEncoder()\n",
    "y = en.fit_transform(y).toarray()\n",
    "\n",
    "print(x.shape)\n",
    "print(x_letter.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:42.229Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 48\n",
    "x_test = x[-test_size:]\n",
    "x = x[:-48]\n",
    "\n",
    "y_test = y[-test_size:]\n",
    "y = y[:-48]\n",
    "\n",
    "print(x.shape)\n",
    "print(x_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:42.362Z"
    }
   },
   "outputs": [],
   "source": [
    "image_generator = ImageDataGenerator(width_shift_range=0.1,\n",
    "                                     height_shift_range=0.1, \n",
    "                                     zoom_range=[0.8,1.2],\n",
    "                                     shear_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:42.487Z"
    }
   },
   "outputs": [],
   "source": [
    "x_total = x.copy()\n",
    "def augment(x):\n",
    "    aug_list = []\n",
    "    for i in range(x.shape[0]):\n",
    "        num_aug = 0\n",
    "        tmp = x[i]\n",
    "        tmp = tmp.reshape((1,) + tmp.shape)\n",
    "        for x_aug in image_generator.flow(tmp, batch_size = 1) :\n",
    "            if num_aug >= 1:\n",
    "                break\n",
    "            aug_list.append(x_aug[0])\n",
    "            num_aug += 1\n",
    "    aug_list = np.array(aug_list)\n",
    "    return aug_list\n",
    "\n",
    "n = 2\n",
    "for i in range(n):\n",
    "    arr = augment(x)\n",
    "    x_total = np.concatenate((x_total, arr), axis=0)\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "print(x_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:42.609Z"
    }
   },
   "outputs": [],
   "source": [
    "y_total = y.copy()\n",
    "for i in range(n):\n",
    "    arr = y.copy()\n",
    "    y_total = np.concatenate((y_total, arr), axis=0)\n",
    "\n",
    "print(y_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:43.035Z"
    }
   },
   "outputs": [],
   "source": [
    "def Conv_block(x, growth_rate, activation='relu'):\n",
    "    x_l = BatchNormalization()(x)\n",
    "    x_l = Activation(activation)(x_l)\n",
    "    x_l = Conv2D(growth_rate*4, (1,1), padding='same', kernel_initializer='he_normal')(x_l)\n",
    "    \n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation(activation)(x_l)\n",
    "    x_l = Conv2D(growth_rate, (3,3), padding='same', kernel_initializer='he_normal')(x_l)\n",
    "    \n",
    "    x = concatenate([x, x_l])\n",
    "    return x\n",
    "\n",
    "def Dense_block(x, layers, growth_rate=32):\n",
    "    for i in range(layers):\n",
    "        x = Conv_block(x, growth_rate)\n",
    "    return x\n",
    "\n",
    "def Transition_layer(x, compression_factor=0.5, activation='relu'):\n",
    "    reduced_filters = int(tf.keras.backend.int_shape(x)[-1] * compression_factor)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Conv2D(reduced_filters, (1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    x = AveragePooling2D((2,2), padding='same', strides=2)(x)\n",
    "    return x\n",
    "\n",
    "def DenseNet(densenet_type='DenseNet-121', base_growth_rate = 32):\n",
    "    model_input = Input(shape=(28,28,1))\n",
    "    x = Conv2D(base_growth_rate*2, (5,5), padding='same', strides=1,\n",
    "               kernel_initializer='he_normal')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2,2), padding='same', strides=1)(x)\n",
    "    \n",
    "    x = Dense_block(x, layers_in_block[densenet_type][0], base_growth_rate)\n",
    "    x = Transition_layer(x, compression_factor=0.5)\n",
    "    x = Dense_block(x, layers_in_block[densenet_type][1], base_growth_rate)\n",
    "    x = Transition_layer(x, compression_factor=0.5)\n",
    "    x = Dense_block(x, layers_in_block[densenet_type][2], base_growth_rate)\n",
    "    #x = Transition_layer(x, compression_factor=0.5)\n",
    "    #x = Dense_block(x, layers_in_block[densenet_type][3], base_growth_rate)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    model_output = Dense(10, activation='softmax', kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    model = Model(model_input, model_output, name=densenet_type)\n",
    "    model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:43.524Z"
    }
   },
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-25T02:05:44.213Z"
    }
   },
   "outputs": [],
   "source": [
    "layers_in_block = {'DenseNet-121':[6, 12, 24, 16],\n",
    "                   'DenseNet-169':[6, 12, 32, 32],\n",
    "                   'DenseNet-201':[6, 12, 48, 32],\n",
    "                   'DenseNet-265':[6, 12, 64, 48]}\n",
    "\n",
    "DN_model = DenseNet('DenseNet-121', 32)\n",
    "CNN_model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T01:41:02.016647Z",
     "start_time": "2020-08-25T01:41:01.999937Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stacking_data(model, x_train, y_train, x_test, n_folds=5):\n",
    "    kfold = KFold(n_splits = n_folds)\n",
    "    \n",
    "    # 최종 모델에서 사용할 데이터셋 셋팅(0 값으로)\n",
    "    # 만약 shape가 (100, 10) 이었으면 폴드의 검증 과정에서 저장할 데이터는 (100, 1)\n",
    "    # 모양을 갖게 한다.\n",
    "    train_fold_predict = np.zeros((x_train.shape[0], 1))\n",
    "    \n",
    "    # test는 x_test 값을 이용해서 매 폴드마다 예측을 하기 때문에 (100, fold개수)\n",
    "    # 만큼의 shape를 갖게 된다.\n",
    "    # 그래서 해당 폴드마다 x_test의 예측 값을 해당 fold에 해당되는 열에 넣는다.\n",
    "    test_predict = np.zeros((x_test.shape[0], n_folds))\n",
    "    print(\"model : \", model.__class__.__name__)\n",
    "    \n",
    "    for cnt, (train_index, valid_index) in enumerate(kfold.split(x_train)):\n",
    "        x_train_ = x_train[train_index]\n",
    "        y_train_ = y_train[train_index]\n",
    "        x_validation = x_train[valid_index]\n",
    "        \n",
    "        # 학습\n",
    "        model.fit(x_train_, y_train_, batch_size=64, epochs=30)\n",
    "        \n",
    "        # 해당 폴드에서 학습된 모델에다가 검증 데이터(x_validation)로 예측 후 저장\n",
    "        train_fold_predict[valid_index, :] = model.predict(x_validation).reshape(-1,1)\n",
    "        \n",
    "        # 해당 폴드에서 생성된 모델에게 원본 테스트 데이터(x_test)를 이용해서 예측을 수행하고 저장\n",
    "        test_predict[:, cnt] = model.predict(x_test)\n",
    "    \n",
    "    # for문이 끝나면 test_pred는 평균을 내서 하나로 합친다.\n",
    "    test_predict_mean = np.mean(test_predict, axis=1).reshape(-1,1)\n",
    "    \n",
    "    return train_fold_predict, test_predict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T01:57:03.288396Z",
     "start_time": "2020-08-25T01:41:54.342921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  KerasClassifier\n",
      "Epoch 1/30\n",
      "4800/4800 [==============================] - 54s 11ms/step - loss: 2.3048 - acc: 0.2681\n",
      "Epoch 2/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 1.3328 - acc: 0.5290\n",
      "Epoch 3/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.8956 - acc: 0.6910\n",
      "Epoch 4/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.6220 - acc: 0.7898\n",
      "Epoch 5/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.4777 - acc: 0.8331\n",
      "Epoch 6/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.4320 - acc: 0.8521\n",
      "Epoch 7/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.3188 - acc: 0.8873\n",
      "Epoch 8/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.2909 - acc: 0.8983\n",
      "Epoch 9/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.2502 - acc: 0.9160\n",
      "Epoch 10/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1638 - acc: 0.9431\n",
      "Epoch 11/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1834 - acc: 0.9344\n",
      "Epoch 12/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1999 - acc: 0.9300\n",
      "Epoch 13/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1606 - acc: 0.9433\n",
      "Epoch 14/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1192 - acc: 0.9606\n",
      "Epoch 15/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1248 - acc: 0.9592\n",
      "Epoch 16/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1160 - acc: 0.9606\n",
      "Epoch 17/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1590 - acc: 0.9471\n",
      "Epoch 18/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1126 - acc: 0.9596\n",
      "Epoch 19/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0771 - acc: 0.9723\n",
      "Epoch 20/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0434 - acc: 0.9831\n",
      "Epoch 21/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0787 - acc: 0.9742\n",
      "Epoch 22/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0849 - acc: 0.9729\n",
      "Epoch 23/30\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.0698 - acc: 0.9781\n",
      "Epoch 24/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0513 - acc: 0.9821\n",
      "Epoch 25/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0776 - acc: 0.9771\n",
      "Epoch 26/30\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.0538 - acc: 0.9813\n",
      "Epoch 27/30\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.0891 - acc: 0.9704\n",
      "Epoch 28/30\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.0779 - acc: 0.9742\n",
      "Epoch 29/30\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.0486 - acc: 0.9854\n",
      "Epoch 30/30\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.0371 - acc: 0.9865\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ff5ac29bf289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDN_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDN_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stacking_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDN_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mCNN_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCNN_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stacking_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCNN_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d23b03fed2db>\u001b[0m in \u001b[0;36mget_stacking_data\u001b[1;34m(model, x_train, y_train, x_test, n_folds)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# 해당 폴드에서 학습된 모델에다가 검증 데이터(x_validation)로 예측 후 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtrain_fold_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 해당 폴드에서 생성된 모델에게 원본 테스트 데이터(x_test)를 이용해서 예측을 수행하고 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \"\"\"\n\u001b[0;32m    241\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "DN_train, DN_test = get_stacking_data(DN_model, x_total, y_total, x_test)\n",
    "CNN_train, CNN_test = get_stacking_data(CNN_model, x_total, y_total, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_train = np.concatenate((DN_train, CNN_train), axis=1)\n",
    "new_x_test = np.concatenate((DN_test, CNN_test), axis=1)\n",
    "\n",
    "print(\"원본 : \", x_total.shape, x_test.shape)\n",
    "print(\"새로운 : \", new_x_train.shape, new_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#train_lgb = lgb.Dataset(new_x_train, label=y_total)\n",
    "#params = {'random_seed':seed, 'bagging_seed':seed,\n",
    "#          'feature_fraction_seed':seed, 'data_random_seed':seed,\n",
    "#          'drop_seed':seed,\n",
    "#          'num_iterations':400,\n",
    "#          'boosting_type':'gbdt', 'objective':'regression_l1',\n",
    "#          'learning_rate':0.05, 'num_leaves':100, 'max_depth':-1,\n",
    "#          'bagging_fraction':0.1, 'feature_fraction':0.8,\n",
    "#          'lambda_l1':0.0, 'lambda_l2':15.0, 'max_bin':300}\n",
    "\n",
    "#lgbm = lgb.train(params, train_lgb, num_boost_round=2000, early_stopping_rounds=100,\n",
    "                 verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('./models/{epoch:02d}-{val_acc:.4f}.h5', monitor='val_loss',\n",
    "                     save_best_only=True, mode='min')\n",
    "\n",
    "history = lgb.fit(new_x_train, y_train, callbacks=[cp])\n",
    "stack_pred = lgbm.predict(new_x_test)\n",
    "\n",
    "print(stack_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:54:52.861734Z",
     "start_time": "2020-08-21T09:53:42.944440Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#best_model = load_model('models/DenseNet121_submit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#x_test = test[:,1:]\n",
    "#x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:59:47.586280Z",
     "start_time": "2020-08-21T09:58:44.284891Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#submission = pd.read_csv('data/submission.csv')\n",
    "#submission['digit'] = np.argmax(best_model.predict(x_test), axis=1)\n",
    "#submission.to_csv('data/submission_densenet(0821).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
